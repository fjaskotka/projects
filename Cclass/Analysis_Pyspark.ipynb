{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analysis of C-Class dataset in PySpark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loading the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('model', StringType()),\n",
    "    StructField('year', IntegerType()),\n",
    "    StructField('price', IntegerType()),\n",
    "    StructField('transmission', StringType()),\n",
    "    StructField('mileage', IntegerType()),\n",
    "    StructField('fuelType', StringType()),\n",
    "    StructField('engineSize', DecimalType())\n",
    "])\n",
    "\n",
    "df = spark.read.csv('cclass.csv', header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----+-----+------------+-------+--------+----------+\n|   model|year|price|transmission|mileage|fuelType|engineSize|\n+--------+----+-----+------------+-------+--------+----------+\n| C Class|2020|30495|   Automatic|   1200|  Diesel|         2|\n| C Class|2020|29989|   Automatic|   1000|  Petrol|         2|\n| C Class|2020|37899|   Automatic|    500|  Diesel|         2|\n| C Class|2019|30399|   Automatic|   5000|  Diesel|         2|\n| C Class|2019|29899|   Automatic|   4500|  Diesel|         2|\n| C Class|2020|30999|   Automatic|   1000|  Diesel|         2|\n| C Class|2020|35999|   Automatic|    500|  Diesel|         2|\n| C Class|2019|37990|   Automatic|   1412|  Petrol|         3|\n| C Class|2019|28990|   Automatic|   3569|  Diesel|         2|\n| C Class|2019|28990|   Automatic|   3635|  Diesel|         2|\n| C Class|2013| 9995|   Automatic|  44900|  Petrol|         2|\n| C Class|2012| 6995|   Automatic|  88200|  Diesel|         2|\n| C Class|2012| 7495|   Automatic| 115000|  Diesel|         2|\n| C Class|2011| 8995|   Automatic|  69250|  Diesel|         2|\n| C Class|2015|14995|   Automatic|  49850|  Diesel|         2|\n| C Class|2013| 8595|   Automatic|  82685|  Diesel|         2|\n| C Class|2016|19250|   Semi-Auto|  32506|  Diesel|         2|\n| C Class|2017|23500|   Semi-Auto|  17000|  Diesel|         2|\n| C Class|2018|24995|   Semi-Auto|  17744|  Diesel|         2|\n| C Class|2016|29995|   Semi-Auto|  64291|  Petrol|         4|\n+--------+----+-----+------------+-------+--------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('model', 'string'),\n",
       " ('year', 'int'),\n",
       " ('price', 'int'),\n",
       " ('transmission', 'string'),\n",
       " ('mileage', 'int'),\n",
       " ('fuelType', 'string'),\n",
       " ('engineSize', 'decimal(10,0)')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----+-----+------------+-------+--------+----------+\n|model|year|price|transmission|mileage|fuelType|engineSize|\n+-----+----+-----+------------+-------+--------+----------+\n|    0|   0|    0|           0|      0|       0|         0|\n+-----+----+-----+------------+-------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#examining Missing values\n",
    "df.select([F.count(F.when(F.isnan(c), c)).alias(c)for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3899"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+--------+------------------+-----------------+------------+------------------+--------+-------------------+\n|summary|   model|              year|            price|transmission|           mileage|fuelType|         engineSize|\n+-------+--------+------------------+-----------------+------------+------------------+--------+-------------------+\n|  count|    3899|              3899|             3899|        3899|              3899|    3899|               3899|\n|   mean|    null|2017.3385483457296|23674.28699666581|        null|22395.709156193894|    null|             2.1036|\n| stddev|    null|2.2134156573374724| 8960.21821842348|        null|22630.438425876873|    null|0.41648280841854707|\n|    min| C Class|              1991|             1290|   Automatic|                 1|  Diesel|                  0|\n|    25%|    null|              2016|            17690|        null|              6000|    null|                2.0|\n|    50%|    null|              2018|            22980|        null|             14640|    null|                2.0|\n|    75%|    null|              2019|            28900|        null|             32475|    null|                2.0|\n|    max| C Class|              2020|            88995|   Semi-Auto|            173000|  Petrol|                  6|\n+-------+--------+------------------+-----------------+------------+------------------+--------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing neccesary packages\n"
   ]
  },
  {
   "source": [
    "### Dropping model column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+------------+-------+--------+----------+\n|year|price|transmission|mileage|fuelType|engineSize|\n+----+-----+------------+-------+--------+----------+\n|2020|30495|   Automatic|   1200|  Diesel|         2|\n|2020|29989|   Automatic|   1000|  Petrol|         2|\n|2020|37899|   Automatic|    500|  Diesel|         2|\n|2019|30399|   Automatic|   5000|  Diesel|         2|\n|2019|29899|   Automatic|   4500|  Diesel|         2|\n|2020|30999|   Automatic|   1000|  Diesel|         2|\n|2020|35999|   Automatic|    500|  Diesel|         2|\n|2019|37990|   Automatic|   1412|  Petrol|         3|\n|2019|28990|   Automatic|   3569|  Diesel|         2|\n|2019|28990|   Automatic|   3635|  Diesel|         2|\n|2013| 9995|   Automatic|  44900|  Petrol|         2|\n|2012| 6995|   Automatic|  88200|  Diesel|         2|\n|2012| 7495|   Automatic| 115000|  Diesel|         2|\n|2011| 8995|   Automatic|  69250|  Diesel|         2|\n|2015|14995|   Automatic|  49850|  Diesel|         2|\n|2013| 8595|   Automatic|  82685|  Diesel|         2|\n|2016|19250|   Semi-Auto|  32506|  Diesel|         2|\n|2017|23500|   Semi-Auto|  17000|  Diesel|         2|\n|2018|24995|   Semi-Auto|  17744|  Diesel|         2|\n|2016|29995|   Semi-Auto|  64291|  Petrol|         4|\n+----+-----+------------+-------+--------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "source": [
    "### Examining Skewness"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'price':'skewness'}).show()\n",
    "\n",
    "#If between -0.5 and 0.5 it is fairly symmetrical\n",
    "# if between -1 and 1 it is moderately skwewd\n",
    "#if between smaller than -1 and biger than 1 then highly skwewd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if positively sweked, log transformation can help\n",
    "df = df.withColumn('price_log', F.log10(F.col('price')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "source": [
    "## Normalization of Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min Max Scaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "#First VectorAssembler\n",
    "#Then MinMaxScaler\n",
    "#pipelining\n",
    "# Iterating over columns to be scaled\n",
    "for i in [\"Revenue\",\"No_of_Days\"]:\n",
    "    # VectorAssembler Transformation - Converting column to vector type\n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fitting pipeline on dataframe\n",
    "    df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fabi Normalization function\n",
    "\n",
    "# Function to normalise dataframes\n",
    "def standardize_train_test_data(train_df, test_df, columns):\n",
    "    '''\n",
    "    Add normalised columns to the input dataframe.\n",
    "    formula = [(X - mean) / std_dev]\n",
    "    Inputs : training dataframe, list of column name strings to be normalised\n",
    "    Returns : dataframe with new normalised columns, averages and std deviation dataframes \n",
    "    '''\n",
    "    # Find the Mean and the Standard Deviation for each column\n",
    "    aggExpr = []\n",
    "    aggStd = []\n",
    "    for column in columns:\n",
    "        aggExpr.append(mean(train_df[column]).alias(column))\n",
    "        aggStd.append(stddev(train_df[column]).alias(column + '_stddev'))\n",
    "    \n",
    "    averages = train_df.agg(*aggExpr).collect()[0]\n",
    "    std_devs = train_df.agg(*aggStd).collect()[0]\n",
    "    \n",
    "    # Standardise each dataframe, column by column\n",
    "    for column in columns:            \n",
    "        # Standardise the TRAINING data\n",
    "        train_df = train_df.withColumn(column + '_norm', ((train_df[column] - averages[column]) / \n",
    "                                                              std_devs[column + '_stddev']))       \n",
    "    \n",
    "        # Standardise the TEST data (using the training mean and std_dev)     \n",
    "        test_df = test_df.withColumn(column + '_norm', ((test_df[column] - averages[column]) / \n",
    "                                                              std_devs[column + '_stddev']))  \n",
    "    return train_df, test_df, averages, std_devs\n"
   ]
  },
  {
   "source": [
    "### One Hot Encoding of categorical variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The number of input columns 1 must be the same as the number of output columns 2.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-45f5295041e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Training pipeline on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#Making predictions on testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of input columns 1 must be the same as the number of output columns 2."
     ]
    }
   ],
   "source": [
    "#Converting categorical strings to index values\n",
    "indexer_trans = StringIndexer(inputCol = 'transmission', outputCol = 'trans_idx')\n",
    "indexer_fuel = StringIndexer(inputCol = 'fuelType', outputCol = 'fuel_idx')\n",
    "\n",
    "\n",
    "#One-Hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols = ['trans,idx, fuel_idx'],\n",
    "    outputCols = ['trans_dummy', 'fuel_dummy']\n",
    ")\n",
    "\n",
    "#Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols = ['year', 'mileage', 'engineSize', 'trans_dummy', 'fuel_dummy'], outputCol = 'features')\n",
    "\n",
    "#Linear regression object\n",
    "regression = LinearRegression(labelCol = 'price')\n",
    "\n",
    "#Constructing pipeline\n",
    "pipeline = Pipeline(stages = [indexer_trans, indexer_fuel, onehot, assembler, regression])\n",
    "\n",
    "#Training pipeline on training data\n",
    "pipeline = pipeline.fit(train)\n",
    "\n",
    "#Making predictions on testing data\n",
    "predictions = pipeline.transform(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}